{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88tCXKMAp2FX"
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"> \n",
    "\n",
    "#  APIs for financial data - Solutions\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "For both parts of this lab we will use Python to interact with the Alpha Vantage API and export data from different endpoints to answer two research questions.\n",
    "\n",
    "# Part 1: GDP and Consumer Sentiment during the pandemic\n",
    "\n",
    "You have been tasked with analysing how various economic indicators have behaved during the last few years. Your team has already identified a free source of this data: Alpha Vantage (https://www.alphavantage.co). It is now up to you to extract the relevant data about these indicators and write a short report about your findings.\n",
    "\n",
    "***Note: be sure to sign up for a free API key at https://www.alphavantage.co/support/#api-key (more detailed instructions are in the instruction document for this lab)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Imports**\n",
    "Put all Python imports in the cell below. If you later decide you need to import something, you must put it here and re-run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet pandas requests    \n",
    "\n",
    "# pip install --quiet pandas requests is for Jupyter notebooks to install pandas and requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO \n",
    "# io is for handling byte streams, BytesIO allows us to read bytes as a file-like object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88tCXKMAp2FX"
   },
   "source": [
    "**1. Find the correct API endpoint to retrieve historical data on GDP**\n",
    "\n",
    "Define a variable `base_url` that is a **string** that is simply the base portion of the Alpha Vantage URL endpoint. That is, if you look at any API call, this is everything before the question mark.\n",
    "\n",
    "Next, define another variable `API_KEY` that is a **string** and is your Alpha Vantage API key (make one, they're free and you don't need to give a real email!)\n",
    "\n",
    "Use the [Documentation](https://www.alphavantage.co/documentation/) to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Alpha Vantage key from a simple text file (alpha.key) \n",
    "\n",
    "with open(\"alpha.key\", \"r\", encoding=\"utf-8\") as f:\n",
    "    API_KEY = f.read().strip()\n",
    "\n",
    "if not API_KEY:\n",
    "    raise EnvironmentError(\"alpha.key is empty. Put only real API key (no quotes) on one line.\")\n",
    "\n",
    "# with open is used to read the API key from a file named alpha.key\n",
    "# r is for read mode\n",
    "# encoding=\"utf-8\" ensures the file is read with UTF-8 encoding\n",
    "# f.read().strip() reads the content of the file and removes any leading/trailing whitespace\n",
    "# If the API key is empty, an EnvironmentError is raised with a message indicating the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Set the base URL \n",
    "BASE_URL = \"https://www.alphavantage.co/query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup OK\n",
      "BASE_URL: https://www.alphavantage.co/query\n"
     ]
    }
   ],
   "source": [
    "# To perform confirmation (no API key in output)\n",
    "assert isinstance(API_KEY, str) and API_KEY.strip(), \"Missing or empty API key.\"   \n",
    "\n",
    "# assert isinstance is a built-in function that checks the type of a variable, \n",
    "# in here we check if API_KEY is a non-empty string and raise an error with a message if not.\n",
    "\n",
    "print(\"Setup OK\")                   # to indicate setup is complete\n",
    "print(\"BASE_URL:\", BASE_URL)           # print the base URL for verification of setup for next steps to retrieve data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0ZTBm3Htt4Ob"
   },
   "outputs": [],
   "source": [
    "# As confirmed from the outputs above, the setup is complete and the base URL is correctly set for Alpha Vantage API.\n",
    "# I will just provide the BASE_URL and API_KEY variables here for ease of reference for the next steps. \n",
    "\n",
    "BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "API_KEY = \"MY_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OstJPrVXt4Ob"
   },
   "source": [
    "**2. Use Python and the `requests` library to make an API call and retrieve historical GDP data at *the highest level of granularity* (i.e. most frequent that the API allows). Remember to add your own API key to the query.**\n",
    "\n",
    "You should convert the results to JSON so it behaves like a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! GDP JSON retrieved.\n",
      "Top-level keys: ['name', 'interval', 'unit', 'data']\n",
      "Number of GDP records: 94\n",
      "First 5 items (raw): [{'date': '2025-04-01', 'value': '5943.384'}, {'date': '2025-01-01', 'value': '5776.724'}, {'date': '2024-10-01', 'value': '5997.184'}, {'date': '2024-07-01', 'value': '5884.567'}, {'date': '2024-04-01', 'value': '5829.384'}]\n"
     ]
    }
   ],
   "source": [
    "# I will call the REAL_GDP endpoint (quarterly) and keep the RAW JSON only\n",
    "# GDP endpoint (Quarterly) because it's the highest granularity available as per docs.\n",
    "# I will request JSON (default) so I can inspect the structure before converting.\n",
    "# I will store the JSON in gdp_json for the next step.\n",
    "# I will print a tiny summary (keys, record count, first 5 items) to confirm the payload.\n",
    "\n",
    "import requests\n",
    "\n",
    "# I will build the request parameters dictionary for the API call\n",
    "# I am NOT setting datatype=csv here because I want JSON for inspection in Step 3\n",
    "gdp_params = {\n",
    "    \"function\": \"REAL_GDP\",     # GDP endpoint\n",
    "    \"interval\": \"quarterly\",    # highest granularity available is quarterly as per docs\n",
    "    \"apikey\": API_KEY,          # my API key from alpha.key\n",
    "    }                           \n",
    "\n",
    "# I will make the API request to get the raw GDP JSON\n",
    "gdp_resp = requests.get(BASE_URL, params=gdp_params, timeout=30)    # timeout=30 seconds to avoid hanging indefinitely\n",
    "gdp_resp.raise_for_status()                                         # raise_for_status will raise an error for HTTP errors\n",
    "\n",
    "# I will parse the JSON\n",
    "gdp_json = gdp_resp.json()\n",
    "\n",
    "# I will do a minimal sanity check and a tiny summary printout\n",
    "if \"data\" not in gdp_json:\n",
    "    msg = gdp_json.get(\"Note\") or gdp_json.get(\"Information\") or gdp_json.get(\"Error Message\") or \"Unexpected response shape.\"\n",
    "    raise RuntimeError(f\"Alpha Vantage did not return 'data'. Hint: {msg}\")\n",
    "\n",
    "print(\"Great! GDP JSON retrieved.\")                      # to confirm successful retrieval\n",
    "print(\"Top-level keys:\", list(gdp_json.keys()))          # to list the top-level keys in the JSON\n",
    "print(\"Number of GDP records:\", len(gdp_json[\"data\"]))   # to count the number of records in the 'data' key\n",
    "print(\"First 5 items (raw):\", gdp_json[\"data\"][:5])      # to print the first 5 items in the 'data' key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhEjJHEat4Oc"
   },
   "source": [
    "**3. Convert the data in this JSON to a Pandas dataframe and export to a csv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wOwGWI_Wt4Oc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['date', 'value']\n",
      "\n",
      "First 5 rows:\n",
      "      date    value\n",
      "2002-01-01 3501.118\n",
      "2002-04-01 3608.496\n",
      "2002-07-01 3650.253\n",
      "2002-10-01 3712.845\n",
      "2003-01-01 3582.767\n",
      "\n",
      "Random 5 rows (to spot oddities):\n",
      "      date    value\n",
      "2012-01-01 4279.816\n",
      "2007-07-01 4208.612\n",
      "2015-10-01 4799.354\n",
      "2020-01-01 5055.712\n",
      "2002-01-01 3501.118\n",
      "\n",
      "Data types (pandas dtypes):\n",
      "date     datetime64[ns]\n",
      "value           float64\n",
      "dtype: object\n",
      "\n",
      "Date range: 2002-01-01 → 2025-04-01\n"
     ]
    }
   ],
   "source": [
    "# From the above output, we can confirm that the GDP JSON has been successfully retrieved and contains the expected data.\n",
    "# I will convert the GDP JSON to a tidy DataFrame, inspect, and save as CSV\n",
    "# I will turn gdp_json[\"data\"] (list of dicts) into a pandas DataFrame.\n",
    "# I will clean headers, parse the date and value columns, and sort by date.\n",
    "# I will inspect columns, first 5 rows, 5 random rows, and dtypes.\n",
    "# I will save the result as gdp_quarterly.csv for Tableau.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# I will make a DataFrame from the JSON payload\n",
    "gdp_df = pd.DataFrame(gdp_json[\"data\"]).copy()\n",
    "\n",
    "# I will clean the column names to snake_case for consistency\n",
    "# I will strip whitespace, convert to lowercase, and replace spaces with underscores\n",
    "gdp_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in gdp_df.columns]\n",
    "\n",
    "# I will parse data types (date to datetime, value to numeric)\n",
    "# I will use errors=\"coerce\" to handle any parsing issues gracefully\n",
    "gdp_df[\"date\"] = pd.to_datetime(gdp_df[\"date\"], errors=\"coerce\")\n",
    "gdp_df[\"value\"] = pd.to_numeric(gdp_df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "# I will sort by date and reset the index for a clean DataFrame\n",
    "# date sorting ensures chronological order and reset_index(drop=True) removes the old index\n",
    "gdp_df = gdp_df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# I will inspect the cleaned DataFrame before saving\n",
    "print(\"Columns:\", list(gdp_df.columns))                        # to list the column names\n",
    "print(\"\\nFirst 5 rows:\")                                       # to print the first 5 rows\n",
    "print(gdp_df.head(5).to_string(index=False))                   # to display the first 5 rows without index\n",
    "print(\"\\nRandom 5 rows (to spot oddities):\")                   # to print 5 random rows for spot-checking\n",
    "print(gdp_df.sample(5, random_state=42).to_string(index=False)) # to display 5 random rows without index\n",
    "print(\"\\nData types (pandas dtypes):\")                         # to print the data types of each column\n",
    "print(gdp_df.dtypes)                                           # to display the data types of each column\n",
    "print(\"\\nDate range:\", gdp_df[\"date\"].min().date(), \"→\", gdp_df[\"date\"].max().date())    # to show the date range in the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Job! Saved gdp_quarterly.csv\n",
      "Rows: 94 | Columns: ['date', 'value']\n",
      "\n",
      "Last 5 rows:\n",
      "      date    value\n",
      "2024-04-01 5829.384\n",
      "2024-07-01 5884.567\n",
      "2024-10-01 5997.184\n",
      "2025-01-01 5776.724\n",
      "2025-04-01 5943.384\n"
     ]
    }
   ],
   "source": [
    "# Above inspection looks good and there are no obvious issues.\n",
    "# Therefore, I will save the cleaned DataFrame to a CSV file \n",
    "gdp_df.to_csv(\"gdp_quarterly.csv\", index=False)\n",
    "\n",
    "# I will print a small confirmation and a tiny preview\n",
    "print(\"Good Job! Saved gdp_quarterly.csv\")                            # to confirm the file has been saved\n",
    "print(\"Rows:\", len(gdp_df), \"| Columns:\", list(gdp_df.columns))  # to print the number of rows and list of columns\n",
    "print(\"\\nLast 5 rows:\")                                          # to print the last 5 rows\n",
    "print(gdp_df.tail(5).to_string(index=False))                     # to display the last 5 rows without index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Next, identify the endpoint for finding daily stock price values and query the VXX ticker.**\n",
    "\n",
    "Make sure to get as much data as possible using the `outputsize` parameter at this endpoint.\n",
    "\n",
    "VXX is a mutual fund that that adequately represents the [VIX](https://www.investopedia.com/articles/optioninvestor/09/implied-volatility-contrary-indicator.asp#:~:text=VIX%20measures%20the%20market%27s%20expectation%20of%20volatility%20over,trends%20in%20the%20VIX%20can%20inform%20trading%20strategies.) index that represents the fear and volatility in the market. When VIX (or VXX) is high, fear controls the market, and when VIX (or VXX) is low, people have more confidence in the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
      "\n",
      "First 5 rows:\n",
      " timestamp  open   high    low  close   volume\n",
      "2025-10-17 39.22 40.825 36.770  36.95 19695614\n",
      "2025-10-16 36.22 40.240 36.075  39.97 26356261\n",
      "2025-10-15 35.27 37.468 35.010  36.58 10753821\n",
      "2025-10-14 36.59 37.815 34.980  36.15 13616518\n",
      "2025-10-13 35.36 35.690 34.410  34.62 11007351\n",
      "\n",
      "Random 5 rows (to spot oddities):\n",
      " timestamp  open   high   low  close   volume\n",
      "2023-02-07 11.69  11.71 11.14  11.24  8694257\n",
      "2018-07-06 36.03  36.17 34.15  34.23 34722772\n",
      "2010-12-13 37.63  38.91 37.59  38.69  9328000\n",
      "2010-12-10 37.96  38.50 37.73  38.13  8243200\n",
      "2009-04-20 94.59 100.12 93.84  99.59   646400\n",
      "\n",
      "Data types (pandas dtypes):\n",
      "timestamp     object\n",
      "open         float64\n",
      "high         float64\n",
      "low          float64\n",
      "close        float64\n",
      "volume         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# I will fetch VXX (daily) and only inspect what came back\n",
    "# I will request CSV and load it into pandas.\n",
    "# I will list all columns, show 5 example rows, and print data types.\n",
    "# I will NOT drop/convert/save yet; this step is just to understand the raw shape.\n",
    "\n",
    "# I will build the request parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",  # This is the daily time series endpoint for daily stock/ETF prices\n",
    "    \"symbol\": \"VXX\",                  # I am querying the VXX ETN\n",
    "    \"outputsize\": \"full\",             # I want the full available history\n",
    "    \"datatype\": \"csv\",                # I want the response in CSV format\n",
    "    \"apikey\": API_KEY,                # my API key loaded earlier from alpha.key\n",
    "}\n",
    "\n",
    "# I will make the API request and load the CSV into a DataFrame\n",
    "resp = requests.get(BASE_URL, params=params, timeout=30)\n",
    "vxx_raw = pd.read_csv(BytesIO(resp.content))\n",
    "\n",
    "# I will inspect structure before any cleaning\n",
    "print(\"Columns:\", list(vxx_raw.columns))                          # to list all column names\n",
    "print(\"\\nFirst 5 rows:\")                                          # to label the preview of the first 5 rows\n",
    "print(vxx_raw.head(5).to_string(index=False))                     # to print the first 5 rows of the DataFrame without the index\n",
    "print(\"\\nRandom 5 rows (to spot oddities):\")                      # to label the preview of 5 random rows\n",
    "print(vxx_raw.sample(5, random_state=42).to_string(index=False))  # to print 5 random rows of the DataFrame without the index for spotting any oddities, using random_state for reproducibility\n",
    "print(\"\\nData types (pandas dtypes):\")                            # to label the data types output\n",
    "print(vxx_raw.dtypes)                                             # to print the data types of each column in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Cast the result as a dataframe and export it to a csv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job! Saved vxx_daily.csv\n",
      "Rows: 4206 | Columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
      "Date range: 2009-01-30 to 2025-10-17\n",
      "\n",
      "Last 5 rows:\n",
      " timestamp  open   high    low  close   volume\n",
      "2025-10-13 35.36 35.690 34.410  34.62 11007351\n",
      "2025-10-14 36.59 37.815 34.980  36.15 13616518\n",
      "2025-10-15 35.27 37.468 35.010  36.58 10753821\n",
      "2025-10-16 36.22 40.240 36.075  39.97 26356261\n",
      "2025-10-17 39.22 40.825 36.770  36.95 19695614\n"
     ]
    }
   ],
   "source": [
    "# From my inspection step, I noticed all columns are required since they contain price/volume data.\n",
    "# Hence, I will keep all 6 columns: timestamp, open, high, low, close, volume.\n",
    "# I will take vxx_raw from Step 4 and finish a tidy version for analysis steps and visualization in Tableau later.\n",
    "# I will copy vxx_raw so I don't accidentally change the original inspection DataFrame.\n",
    "# I will convert timestamp to a real date and price/volume columns to numeric.\n",
    "# I will drop duplicate dates (safety), sort oldest to the newest, and save to vxx_daily.csv.\n",
    "\n",
    "# I will make a working copy of the raw DataFrame from step 4\n",
    "vxx = vxx_raw[[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]].copy()\n",
    "\n",
    "# I will convert the date and numeric columns\n",
    "vxx[\"timestamp\"] = pd.to_datetime(vxx[\"timestamp\"], errors=\"coerce\") \n",
    "# I parse the date column by making timestamp a datetime type, errors=\"coerce\" will set invalid parsing as NaT\n",
    "\n",
    "for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "    vxx[col] = pd.to_numeric(vxx[col], errors=\"coerce\")\n",
    "# I convert price/volume columns to numeric, errors=\"coerce\" will set invalid parsing as NaN\n",
    "\n",
    "# I will drop exact duplicate dates (just in case) and sort by date\n",
    "vxx = (\n",
    "    vxx.drop_duplicates(subset=[\"timestamp\"])    # drop duplicate dates based on timestamp\n",
    "       .sort_values(\"timestamp\")                 # sort by timestamp from oldest to newest\n",
    "       .reset_index(drop=True)                   # reset index after sorting, drop=True to avoid adding old index as a column\n",
    ")\n",
    "\n",
    "# I will save the cleaned DataFrame to a CSV file \n",
    "vxx.to_csv(\"vxx_daily.csv\", index=False)         # save to vxx_daily.csv without the index column\n",
    "\n",
    "# I will print a small summary and preview\n",
    "print(\"Good job! Saved vxx_daily.csv\")                                  # to confirm the file has been saved\n",
    "print(\"Rows:\", len(vxx), \"| Columns:\", list(vxx.columns))               # to print the number of rows and list of columns\n",
    "print(\"Date range:\", vxx[\"timestamp\"].min().date(), \"to\", vxx[\"timestamp\"].max().date())   # to print the date range in the data\n",
    "print(\"\\nLast 5 rows:\")                                                 # to label the preview of the last 5 rows\n",
    "print(vxx.tail(5).to_string(index=False))                               # to display the last 5 rows without index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Repeat steps 4 and 5, but this time for VTI**\n",
    "\n",
    "Use all of the same settings to now gather data for VTI, a mutual fund that consists of _every_ US stock. VTI can be considered an indicator for the overall market.\n",
    "\n",
    "Again, as in step 5, save this data out to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
      "\n",
      "First 5 rows:\n",
      " timestamp    open    high      low  close  volume\n",
      "2025-10-17 325.010 327.940 324.2400 327.30 5646772\n",
      "2025-10-16 329.370 329.935 324.0800 325.77 5328475\n",
      "2025-10-15 329.390 330.990 325.2950 328.38 4245557\n",
      "2025-10-14 323.900 328.620 322.0800 326.83 6307297\n",
      "2025-10-13 325.785 327.580 325.1632 326.93 4476790\n",
      "\n",
      "Random 5 rows (to spot oddities):\n",
      " timestamp   open    high    low  close  volume\n",
      "2010-12-22  64.81  65.020  64.77  64.93 2097600\n",
      "2021-01-08 199.14 199.460 196.98 199.25 7538654\n",
      "2005-04-15 112.73 112.930 111.08 111.10  223800\n",
      "2023-10-18 215.11 215.630 212.59 213.09 3254961\n",
      "2023-02-07 206.00 209.563 205.03 208.88 3028103\n",
      "\n",
      "Data types (pandas dtypes):\n",
      "timestamp     object\n",
      "open         float64\n",
      "high         float64\n",
      "low          float64\n",
      "close        float64\n",
      "volume         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# I will break Step 6 into two parts for clarity (step 6.1 and 6.2)\n",
    "# This will be Step 6.1 and I will perform the inspection only.\n",
    "# I will fetch VTI (daily) and only inspect what came back\n",
    "# I will use TIME_SERIES_DAILY with outputsize=\"full\".\n",
    "# I will request CSV so it loads easily into pandas.\n",
    "# I will inspect columns, first 5 rows, random 5 rows, and dtypes.\n",
    "# I will NOT clean or save here; that will be the next step 6.2.\n",
    "\n",
    "\n",
    "# I will build the request parameters for the API call\n",
    "vti_params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",  # To request time series daily for stock/ETF price data\n",
    "    \"symbol\": \"VTI\",                  # To request data for the VTI ETF\n",
    "    \"outputsize\": \"full\",             # To get as much history as possible\n",
    "    \"datatype\": \"csv\",                # To get CSV for easy loading\n",
    "    \"apikey\": API_KEY,                # my API key from alpha.key\n",
    "}\n",
    "\n",
    "# I will make the API request and load the CSV into a DataFrame\n",
    "vti_resp = requests.get(BASE_URL, params=vti_params, timeout=30)\n",
    "# To avoid hanging indefinitely, I set timeout=30 seconds\n",
    "vti_raw = pd.read_csv(BytesIO(vti_resp.content))\n",
    "# To load the CSV content into a pandas DataFrame \n",
    "\n",
    "# I will inspect the raw DataFrame before any cleaning\n",
    "print(\"Columns:\", list(vti_raw.columns))                       # to list the column names\n",
    "print(\"\\nFirst 5 rows:\")                                       # to print the first 5 rows\n",
    "print(vti_raw.head(5).to_string(index=False))                  # to display the first 5 rows without index\n",
    "print(\"\\nRandom 5 rows (to spot oddities):\")                   # to print 5 random rows for spot-checking\n",
    "print(vti_raw.sample(5, random_state=42).to_string(index=False))  # to display 5 random rows without index\n",
    "print(\"\\nData types (pandas dtypes):\")                         # to print the data types of each column\n",
    "print(vti_raw.dtypes)                                          # to display the data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Job! Saved vti_daily.csv\n",
      "Rows: 6133 | Columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
      "Date range: 2001-05-31 → 2025-10-17\n",
      "\n",
      "Last 5 rows:\n",
      " timestamp    open    high      low  close  volume\n",
      "2025-10-13 325.785 327.580 325.1632 326.93 4476790\n",
      "2025-10-14 323.900 328.620 322.0800 326.83 6307297\n",
      "2025-10-15 329.390 330.990 325.2950 328.38 4245557\n",
      "2025-10-16 329.370 329.935 324.0800 325.77 5328475\n",
      "2025-10-17 325.010 327.940 324.2400 327.30 5646772\n"
     ]
    }
   ],
   "source": [
    "# From my inspection step 6.1, I noticed all columns are required since they contain price/volume data.\n",
    "# Hence, I will keep all 6 columns: timestamp, open, high, low, close, volume.\n",
    "# I will tidy VTI and save a clean CSV for analysis and visualization later.\n",
    "# I will convert timestamp to datetime and prices/volume to numeric.\n",
    "# I will drop duplicate dates (safety), sort oldest to newest, and save vti_daily.csv.\n",
    "# I will print a short summary and a tiny preview.\n",
    "\n",
    "# I will make a working copy of the raw DataFrame from above step 6.1.\n",
    "vti = vti_raw[[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]].copy()\n",
    "\n",
    "# I will convert the date and numeric columns\n",
    "vti[\"timestamp\"] = pd.to_datetime(vti[\"timestamp\"], errors=\"coerce\")\n",
    "# I parse the date column by making timestamp a datetime type, errors=\"coerce\" will set invalid parsing as NaT\n",
    "\n",
    "for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "    vti[col] = pd.to_numeric(vti[col], errors=\"coerce\")\n",
    "# I convert price/volume columns to numeric, errors=\"coerce\" will set invalid parsing as NaN\n",
    "\n",
    "# I will drop exact duplicate dates and sort by date\n",
    "vti = (\n",
    "    vti.drop_duplicates(subset=[\"timestamp\"])   # drop duplicate dates based on timestamp\n",
    "       .sort_values(\"timestamp\")                # sort by timestamp from oldest to newest\n",
    "       .reset_index(drop=True)                  # reset index after sorting, drop=True to avoid adding old index as a column\n",
    ")\n",
    "\n",
    "# I will save the cleaned DataFrame to a CSV file\n",
    "vti.to_csv(\"vti_daily.csv\", index=False)      # save to vti_daily.csv without the index column\n",
    "\n",
    "# I will print a small summary and preview\n",
    "print(\"Good Job! Saved vti_daily.csv\")                                     # to confirm the file has been saved\n",
    "print(\"Rows:\", len(vti), \"| Columns:\", list(vti.columns))           # to print the number of rows and list of columns\n",
    "print(\"Date range:\", vti[\"timestamp\"].min().date(), \"→\", vti[\"timestamp\"].max().date())  # to print the date range in the data\n",
    "print(\"\\nLast 5 rows:\")                                             # to label the preview of the last 5 rows\n",
    "print(vti.tail(5).to_string(index=False))                           # to display the last 5 rows without index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MJrJyq5t4Od"
   },
   "source": [
    "# Part 2: Cryptocurrencies\n",
    "\n",
    "Your stakeholders are becoming aware of the rise in cryptocurrencies, and would like to understand the recent growth of this market. Your task is to use the Alpha Vantage API to extract historical data on cryptocurrency market performance, and tell a story about their growth using visuals created in a BI tool of your choice (Tableau/Power BI).\n",
    "\n",
    "**1. Find the correct API endpoints to retrieve historical data on cryptocurrency prices over time. Daily should be a sufficient level of granularity for your purposes.**\n",
    "\n",
    "Use any specific cryptocurrency you wish (e.g. Bitcoin) against the US Dollar.\n",
    "\n",
    "Use the [Documentation](https://www.alphavantage.co/documentation/) to help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvQWUQ6Et4Od"
   },
   "source": [
    "**2. Use Python to read the data as JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! BTC WEEKLY JSON retrieved.\n"
     ]
    }
   ],
   "source": [
    "# My cryptocurrency of choice is Bitcoin (BTC) for this project.\n",
    "# I choose BTC because it is the most well-known and widely used cryptocurrency with high liquidity and it has a rich historical data set.\n",
    "# It is also often used as a benchmark for the overall cryptocurrency market.\n",
    "# In this step, I will only fetch the RAW JSON for BTC weekly in USD and quickly inspect the structure.\n",
    "# I am fetching BTC weekly compared to daily because daily data only has free tier returns ~ 365 days for BTC-USD as per Alpha Vantage documentation.\n",
    "# To get a few years of data, weekly is more suitable as Alpha Vantage provides more historical data points for weekly intervals.\n",
    "\n",
    "# I will separate Step 2 into two parts for clarity.\n",
    "# Step 2.1 will fetch BTC JSON only for inspection in Step 2.2.\n",
    "# I will keep the JSON in btcw_json for Step 2.2 (deeper inspection) and Step 3 (export).\n",
    "\n",
    "# Part 2 — Step 2.1 \n",
    "\n",
    "import requests\n",
    "\n",
    "# I will build the request parameters for the API call (weekly crypto)\n",
    "btcw_params = {\n",
    "    \"function\": \"DIGITAL_CURRENCY_WEEKLY\",  # to get weekly digital currency data for Bitcoin (Crypto endpoint)\n",
    "    \"symbol\": \"BTC\",                        # BTC = Bitcoin\n",
    "    \"market\": \"USD\",                        # quote currency = US Dollar\n",
    "    \"apikey\": API_KEY,                      # my API key from alpha.key\n",
    "}\n",
    "\n",
    "# I will make the API request to get the raw BTC weekly JSON\n",
    "btcw_resp = requests.get(BASE_URL, params=btcw_params, timeout=30)  # timeout=30 seconds to avoid hanging indefinitely\n",
    "btcw_resp.raise_for_status()                 # raise_for_status will raise an error for HTTP errors\n",
    "btcw_json = btcw_resp.json()                  # btcw_resp.json() parses the JSON response into a Python dictionary for further processing\n",
    "\n",
    "\n",
    "# I will print quick confirmation so I know what I received and can inspect next\n",
    "print(\"Great! BTC WEEKLY JSON retrieved.\")             # to confirm successful retrieval of BTC weekly JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level key count: 2\n",
      "Top-level keys: ['Meta Data', 'Time Series (Digital Currency Weekly)']\n",
      "\n",
      "Latest weekly date in JSON: 2025-10-19\n",
      "Fields on that date: ['1. open', '2. high', '3. low', '4. close', '5. volume']\n"
     ]
    }
   ],
   "source": [
    "# Still Step 2.1 — just confirming what keys exist as previous steps do not provide this info\n",
    "\n",
    "print(\"Top-level key count:\", len(btcw_json))          # to count the number of top-level keys in the JSON for quick overview\n",
    "print(\"Top-level keys:\", list(btcw_json.keys()))       # to list all the top-level keys in the JSON\n",
    "\n",
    "# I will locate the time-series key and peek at the newest date + its fields\n",
    "ts_key_w = \"Time Series (Digital Currency Weekly)\"          # key for weekly digital currency time series data\n",
    "if ts_key_w not in btcw_json:\n",
    "    raise KeyError(f\"Expected key not found: {ts_key_w}\")     # raise KeyError if the expected time-series key is not found in the JSON\n",
    "\n",
    "latest_date = sorted(btcw_json[ts_key_w].keys(), reverse=True)[0]   # to get the latest date by sorting the keys in descending order and taking the first one\n",
    "print(\"\\nLatest weekly date in JSON:\", latest_date)                 # to print the latest weekly date available in the JSON time series data\n",
    "print(\"Fields on that date:\", list(btcw_json[ts_key_w][latest_date].keys()))  # to print the field names for the latest date in the time series data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['date', 'open', 'high', 'low', 'close', 'volume']\n",
      "\n",
      "First 5 rows:\n",
      "      date            open            high             low           close         volume\n",
      "2025-10-19 115067.96000000 116077.51000000 103516.75000000 107035.91000000 59819.95219502\n",
      "2025-10-12 123520.80000000 126296.00000000 107000.00000000 115067.98000000 73716.51126655\n",
      "2025-10-05 112197.66000000 125750.00000000 111582.65000000 123520.79000000 49462.68044524\n",
      "2025-09-28 115282.26000000 115441.37000000 108623.70000000 112197.67000000 38310.34046273\n",
      "2025-09-21 115314.12000000 117998.17000000 114395.84000000 115282.27000000 37222.96619062\n",
      "\n",
      "Random 5 rows (to spot oddities):\n",
      "      date            open            high             low           close          volume\n",
      "2025-07-20 119130.81000000 123231.07000000 115697.37000000 117312.70000000  67814.54474915\n",
      "2024-12-08  97259.17000000 104000.00000000  92055.86000000 101174.99000000 114187.97430796\n",
      "2024-11-24  89877.11000000  99860.00000000  89372.65000000  98028.18000000 137788.93225654\n",
      "2024-12-15 101175.00000000 105100.00000000  94220.80000000 104447.76000000 102016.08653276\n",
      "2025-06-22 105600.21000000 109000.00000000  98225.01000000 100996.87000000  37046.54064646\n",
      "\n",
      "Data types (pandas dtypes):\n",
      "date      object\n",
      "open      object\n",
      "high      object\n",
      "low       object\n",
      "close     object\n",
      "volume    object\n",
      "dtype: object\n",
      "\n",
      "Nulls by column (before cleaning):\n",
      "date      0\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n",
      "\n",
      "String date range: 2024-11-17 to 2025-10-19\n",
      "Rows: 49\n"
     ]
    }
   ],
   "source": [
    "# Above output confirms I have the expected structure and fields for BTC weekly JSON.\n",
    "# Now, Step 2.2 — I will do a deeper inspection of the BTC weekly JSON.\n",
    "# I will turn the weekly BTC JSON (btcw_json) into a quick pandas DataFrame for inspection.\n",
    "# I will not clean or save here; I just want to confirm shape, columns, dtypes, and a string date range.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# I will extract the time series data from the JSON\n",
    "ts = btcw_json[ts_key_w]  # dict: {'YYYY-MM-DD': {'1. open': '...', '2. high': '...'} ...}\n",
    "\n",
    "# I will build records using safe lookups (fields present in weekly payload)\n",
    "def pick(row, *candidates):\n",
    "    # I will return the first available field among the candidates\n",
    "    for c in candidates:\n",
    "        if c in row:\n",
    "            return row[c]\n",
    "    return None\n",
    "\n",
    "records = []\n",
    "for date_str, row in ts.items():\n",
    "    records.append({\n",
    "        \"date\": date_str,                       # keep string for now (I will type it when cleaning)\n",
    "        \"open\":   pick(row, \"1. open\"),         # open price\n",
    "        \"high\":   pick(row, \"2. high\"),         # high price\n",
    "        \"low\":    pick(row, \"3. low\"),          # low price\n",
    "        \"close\":  pick(row, \"4. close\"),        # close price\n",
    "        \"volume\": pick(row, \"5. volume\"),       # reported volume (note: crypto volume units vary by source)\n",
    "    })\n",
    "\n",
    "btcw_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# I will do a light inspection (columns, first 5, random 5, dtypes, null counts, string date range)\n",
    "print(\"Columns:\", list(btcw_df.columns))\n",
    "print(\"\\nFirst 5 rows:\")                        # to label the preview of the first 5 rows\n",
    "print(btcw_df.head(5).to_string(index=False))   # to display the first 5 rows without index\n",
    "\n",
    "print(\"\\nRandom 5 rows (to spot oddities):\")                        # to label the preview of 5 random rows\n",
    "print(btcw_df.sample(5, random_state=42).to_string(index=False))    # to display 5 random rows without index\n",
    "\n",
    "print(\"\\nData types (pandas dtypes):\")                 # to label the data types output\n",
    "print(btcw_df.dtypes)                                  # to print the data types of each column\n",
    "\n",
    "print(\"\\nNulls by column (before cleaning):\")        # to label the null counts output\n",
    "print(btcw_df.isna().sum())                          # to print the count of null values in each column\n",
    "\n",
    "# I will also print the string date range (typed dates will be in the cleaning step)\n",
    "dates = sorted(btcw_df[\"date\"])\n",
    "print(\"\\nString date range:\", dates[0], \"to\", dates[-1])    # to print the string date range from earliest to latest\n",
    "print(\"Rows:\", len(btcw_df))                                # to print the total number of rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! BTC MONTHLY JSON retrieved.\n",
      "Top-level key count: 2\n",
      "Top-level keys (first 5): ['Meta Data', 'Time Series (Digital Currency Monthly)']\n",
      "\n",
      "Latest monthly date in JSON: 2025-10-19\n",
      "Fields on that date: ['1. open', '2. high', '3. low', '4. close', '5. volume']\n"
     ]
    }
   ],
   "source": [
    "# Above inspection looks good for BTC weekly JSON converted to DataFrame shape \n",
    "# But I noticed that I only got approximately 1 year of weekly data (49 rows) from 2024-11-17 to 2025-10-19 due to free tier limitations.\n",
    "\n",
    "# Therefore, I will now proceed to Part 2 — Step 2.3 to check whether I could fetch BTC monthly JSON for a longer history.\n",
    "# Part 2 — Step 2.3 (Monthly)\n",
    "# I will fetch Bitcoin (BTC) monthly data quoted in USD as RAW JSON only.\n",
    "# I will keep the JSON in btcm_json for the next steps (inspection, then export).\n",
    "\n",
    "import requests\n",
    "\n",
    "# I will build the request parameters for the API call (monthly crypto)\n",
    "btcm_params = {\n",
    "    \"function\": \"DIGITAL_CURRENCY_MONTHLY\",   # to get monthly digital currency data for Bitcoin\n",
    "    \"symbol\": \"BTC\",                           # BTC = Bitcoin\n",
    "    \"market\": \"USD\",                           # quote currency = US Dollar\n",
    "    \"apikey\": API_KEY,                         # my API key from alpha.key\n",
    "}\n",
    "\n",
    "# I will make the API request to get the raw BTC monthly JSON\n",
    "btcm_resp = requests.get(BASE_URL, params=btcm_params, timeout=30)  # timeout=30 seconds\n",
    "btcm_resp.raise_for_status()                                         # raise error for HTTP issues\n",
    "btcm_json = btcm_resp.json()                                         # parse JSON into a Python dict\n",
    "\n",
    "# I will print a tiny confirmation so I know what I received and can inspect next\n",
    "print(\"Great! BTC MONTHLY JSON retrieved.\")\n",
    "print(\"Top-level key count:\", len(btcm_json))\n",
    "print(\"Top-level keys (first 5):\", list(btcm_json.keys())[:5])\n",
    "\n",
    "# I will locate the time-series key and peek at the newest date + its fields\n",
    "ts_key_m = \"Time Series (Digital Currency Monthly)\"\n",
    "if ts_key_m not in btcm_json:\n",
    "    raise KeyError(f\"Expected key not found: {ts_key_m}\")\n",
    "\n",
    "latest_date_m = sorted(btcm_json[ts_key_m].keys(), reverse=True)[0]   # newest month\n",
    "print(\"\\nLatest monthly date in JSON:\", latest_date_m)\n",
    "print(\"Fields on that date:\", list(btcm_json[ts_key_m][latest_date_m].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records (months): 11\n",
      "Date range: 2024-12-31 to 2025-10-19\n",
      "\n",
      "Earliest month fields: ['1. open', '2. high', '3. low', '4. close', '5. volume']\n",
      "Earliest month values: {'1. open': '96464.95000000', '2. high': '108388.88000000', '3. low': '91271.19000000', '4. close': '93354.22000000', '5. volume': '422201.07415397'}\n"
     ]
    }
   ],
   "source": [
    "# From the above output, I can confirm that I have successfully retrieved the BTC monthly JSON and it contains the expected structure and fields.\n",
    "# Now, Step 2.4 — I will confirm the BTC MONTHLY history span (earliest month/year, latest month, and count)\n",
    "\n",
    "# I will grab the monthly time series mapping (YYYY-MM-DD -> fields dict)\n",
    "ts_m = btcm_json[ts_key_m]\n",
    "\n",
    "# I will sort keys ascending to find the earliest month\n",
    "all_months = sorted(ts_m.keys())                 # ascending chronological\n",
    "earliest_date_m = all_months[0]                  # earliest month\n",
    "latest_date_m   = all_months[-1]                 # latest month\n",
    "n_months        = len(all_months)                # total number of monthly records\n",
    "\n",
    "# I will print a concise summary\n",
    "print(\"Records (months):\", n_months)\n",
    "print(\"Date range:\", earliest_date_m, \"to\", latest_date_m)\n",
    "\n",
    "# I will also peek at the earliest month’s fields to confirm structure\n",
    "print(\"\\nEarliest month fields:\", list(ts_m[earliest_date_m].keys()))\n",
    "print(\"Earliest month values:\", ts_m[earliest_date_m]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.5 OK — proceeding with BTC DAILY (~1 year, free tier).\n"
     ]
    }
   ],
   "source": [
    "# Step 2.5 — Decision & scope for crypto\n",
    "# After testing the weekly and monthly crypto endpoints, I confirmed Alpha Vantage free tier returns ~1 year of history for crypto. \n",
    "# Daily is the richest of the three, so I will proceed with BTC DAILY and clearly state this limitation in the report.\n",
    "# For fair comparison with VTI and VXX later, I will align to the common last ~365 days.\n",
    "# (The exact window will be derived after we load BTC daily, VTI daily, and VXX daily.)\n",
    "\n",
    "print(\"Step 2.5 OK — proceeding with BTC DAILY (~1 year, free tier).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! BTC DAILY JSON retrieved.\n",
      "Top-level key count: 2\n",
      "Top-level keys (first 5): ['Meta Data', 'Time Series (Digital Currency Daily)']\n",
      "\n",
      "Latest date in JSON: 2025-10-19\n",
      "Fields on that date: ['1. open', '2. high', '3. low', '4. close', '5. volume']\n"
     ]
    }
   ],
   "source": [
    "# Step 2.6 — I will fetch BTC DAILY (JSON only) and do a quick sanity check\n",
    "\n",
    "import requests\n",
    "\n",
    "btcd_params = {\n",
    "    \"function\": \"DIGITAL_CURRENCY_DAILY\",   # daily digital currency endpoint\n",
    "    \"symbol\":   \"BTC\",                       # BTC = Bitcoin\n",
    "    \"market\":   \"USD\",                       # quote currency in USD\n",
    "    \"apikey\":   API_KEY,                     # my Alpha Vantage API key (loaded earlier)\n",
    "}\n",
    "\n",
    "btcd_resp = requests.get(BASE_URL, params=btcd_params, timeout=30)\n",
    "btcd_resp.raise_for_status()                 # raise error for HTTP issues\n",
    "btcd_json = btcd_resp.json()                 # parse JSON into Python dictionary for further processing\n",
    "\n",
    "print(\"Great! BTC DAILY JSON retrieved.\")                    # to confirm successful retrieval\n",
    "print(\"Top-level key count:\", len(btcd_json))                # to count the number of top-level keys\n",
    "print(\"Top-level keys (first 5):\", list(btcd_json.keys())[:5])  # to list the first 5 top-level keys\n",
    "\n",
    "# locate the time-series key and peek at latest date + fields\n",
    "ts_key_d = \"Time Series (Digital Currency Daily)\"       # time-series key for daily digital currency data\n",
    "if ts_key_d not in btcd_json:                    \n",
    "    raise KeyError(f\"Expected key not found: {ts_key_d}\")  # to raise error if expected key is missing\n",
    "\n",
    "latest_date_d = sorted(btcd_json[ts_key_d].keys(), reverse=True)[0]\n",
    "print(\"\\nLatest date in JSON:\", latest_date_d)                                 # to print the latest date available in the JSON time series data\n",
    "print(\"Fields on that date:\", list(btcd_json[ts_key_d][latest_date_d].keys())) # to print the field names for the latest date in the time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1FQgLrmt4Oe"
   },
   "source": [
    "**3. Identify the key which holds the data itself and export it as a csv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['date', 'open_usd', 'high_usd', 'low_usd', 'close_usd', 'volume', 'market_cap_usd']\n",
      "\n",
      "First 5 rows:\n",
      "      date        open_usd        high_usd         low_usd       close_usd         volume market_cap_usd\n",
      "2025-10-19 107208.91000000 107300.00000000 107000.00000000 107035.91000000   107.05717453           None\n",
      "2025-10-18 106462.09000000 107535.68000000 106357.69000000 107208.91000000  3552.60959467           None\n",
      "2025-10-17 108197.99000000 109269.97000000 103516.75000000 106463.30000000 13824.34463474           None\n",
      "2025-10-16 110804.11000000 112021.07000000 107464.53000000 108198.00000000 14792.78754529           None\n",
      "2025-10-15 113072.18000000 113671.99000000 110185.87000000 110804.12000000  7440.03782479           None\n",
      "\n",
      "Random 5 rows (to spot oddities):\n",
      "      date        open_usd        high_usd         low_usd       close_usd         volume market_cap_usd\n",
      "2025-05-15 103540.91000000 104200.00000000 101400.01000000 103786.43000000  7563.00946962           None\n",
      "2024-11-12  88770.74000000  90100.00000000  85010.00000000  88035.43000000 40111.80924643           None\n",
      "2024-12-08  99919.20000000 101435.05000000  98729.66000000 101174.99000000  4230.35225168           None\n",
      "2025-02-27  84111.77000000  86993.87000000  82554.11000000  84625.19000000 16836.55524727           None\n",
      "2025-05-17 103500.03000000 103741.98000000 102612.00000000 103161.07000000  1848.95251021           None\n",
      "\n",
      "Data types (pandas dtypes):\n",
      "date              object\n",
      "open_usd          object\n",
      "high_usd          object\n",
      "low_usd           object\n",
      "close_usd         object\n",
      "volume            object\n",
      "market_cap_usd    object\n",
      "dtype: object\n",
      "\n",
      "Nulls by column (before cleaning):\n",
      "date                0\n",
      "open_usd            0\n",
      "high_usd            0\n",
      "low_usd             0\n",
      "close_usd           0\n",
      "volume              0\n",
      "market_cap_usd    350\n",
      "dtype: int64\n",
      "\n",
      "String date range: 2024-11-04 to 2025-10-19\n",
      "Rows: 350\n"
     ]
    }
   ],
   "source": [
    "# I will separate Step 3 into two parts for clarity : Step 3.1 and Step 3.2.\n",
    "# In Step 3.1, I will build an inspection DataFrame (strings) to confirm shape & ranges.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ts = btcd_json[ts_key_d]   # dict: 'YYYY-MM-DD' -> { '1. open', '2. high', '3. low', '4. close', '5. volume', '6. market cap (USD)' }\n",
    "\n",
    "def pick(row, *candidates):\n",
    "    # returns the first field that exists (handles variants like '1a. open (USD)')\n",
    "    for c in candidates:\n",
    "        if c in row:\n",
    "            return row[c]\n",
    "    return None\n",
    "\n",
    "records = []\n",
    "for date_str, row in ts.items():\n",
    "    records.append({\n",
    "        \"date\":           date_str,                                  # keep as string now; will convert when cleaning \n",
    "        \"open_usd\":       pick(row, \"1a. open (USD)\", \"1. open\"),    \n",
    "        \"high_usd\":       pick(row, \"2a. high (USD)\", \"2. high\"),\n",
    "        \"low_usd\":        pick(row, \"3a. low (USD)\",  \"3. low\"),\n",
    "        \"close_usd\":      pick(row, \"4a. close (USD)\",\"4. close\"),\n",
    "        \"volume\":         pick(row, \"5. volume\"),\n",
    "        \"market_cap_usd\": pick(row, \"6. market cap (USD)\"),\n",
    "    })\n",
    "\n",
    "btc_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# light inspection\n",
    "print(\"Columns:\", list(btc_df.columns))                     # to list the column names\n",
    "print(\"\\nFirst 5 rows:\")                                    # to label the preview of the first 5 rows\n",
    "print(btc_df.head(5).to_string(index=False))                # to display the first 5 rows without index\n",
    "\n",
    "print(\"\\nRandom 5 rows (to spot oddities):\")                # to label the preview of 5 random rows\n",
    "print(btc_df.sample(5, random_state=42).to_string(index=False))  # to display 5 random rows without index\n",
    "\n",
    "print(\"\\nData types (pandas dtypes):\")                      # to label the data types output\n",
    "print(btc_df.dtypes)                                        # to print the data types of each column\n",
    "  \n",
    "print(\"\\nNulls by column (before cleaning):\")                 # to label the null counts output\n",
    "print(btc_df.isna().sum())                                    # to print the count of null values in each column\n",
    "\n",
    "# string date range\n",
    "dates = sorted(btc_df[\"date\"])                                 # sort date strings \n",
    "print(\"\\nString date range:\", dates[0], \"to\", dates[-1])       # to print the string date range from earliest to latest\n",
    "print(\"Rows:\", len(btc_df))                                    # to print the total number of rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job! Saved btc_daily.csv\n",
      "Rows: 350 | Columns: ['date', 'open', 'high', 'low', 'close', 'volume', 'market_cap_usd']\n",
      "Date range: 2024-11-04 → 2025-10-19\n",
      "\n",
      "Last 5 rows:\n",
      "      date      open      high       low     close       volume  market_cap_usd\n",
      "2025-10-15 113072.18 113671.99 110185.87 110804.12  7440.037825             NaN\n",
      "2025-10-16 110804.11 112021.07 107464.53 108198.00 14792.787545             NaN\n",
      "2025-10-17 108197.99 109269.97 103516.75 106463.30 13824.344635             NaN\n",
      "2025-10-18 106462.09 107535.68 106357.69 107208.91  3552.609595             NaN\n",
      "2025-10-19 107208.91 107300.00 107000.00 107035.91   107.057175             NaN\n"
     ]
    }
   ],
   "source": [
    "# From the above inspection, I can confirm that the BTC daily JSON has been successfully converted to a DataFrame and contains the expected structure and fields.\n",
    "# Now, Step 3.2 — I will clean, standardize, and export the BTC DAILY dataset for analysis and Tableau visualization later.\n",
    "\n",
    "# I will make a copy so the raw inspection df remains untouched\n",
    "btc = btc_df.copy()\n",
    "\n",
    "# Rename to align with VTI/VXX style (drop \"_usd\" suffix for OHLC)\n",
    "btc = btc.rename(columns={\n",
    "    \"open_usd\":  \"open\",\n",
    "    \"high_usd\":  \"high\",\n",
    "    \"low_usd\":   \"low\",\n",
    "    \"close_usd\": \"close\",\n",
    "})\n",
    "# 'volume' and 'market_cap_usd' already have good names hence no renaming needed\n",
    "\n",
    "# Parse date and cast numeric columns\n",
    "btc[\"date\"] = pd.to_datetime(btc[\"date\"], errors=\"coerce\") \n",
    "  # I parse the date column by making 'date' a datetime type, errors=\"coerce\" will set invalid parsing as NaT\n",
    "for col in [\"open\", \"high\", \"low\", \"close\", \"volume\", \"market_cap_usd\"]:\n",
    "    if col in btc.columns:\n",
    "        btc[col] = pd.to_numeric(btc[col], errors=\"coerce\")\n",
    "\n",
    "# Drop exact duplicate dates (safety), sort oldest→newest, reset index\n",
    "btc = (\n",
    "    btc.drop_duplicates(subset=[\"date\"])\n",
    "       .sort_values(\"date\")\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Export (no index)\n",
    "btc.to_csv(\"btc_daily.csv\", index=False)\n",
    "\n",
    "# Confirmation + tiny preview\n",
    "print(\"Good job! Saved btc_daily.csv\")\n",
    "print(\"Rows:\", len(btc), \"| Columns:\", list(btc.columns))\n",
    "print(\"Date range:\", btc[\"date\"].min().date(), \"→\", btc[\"date\"].max().date())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(btc.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, CSV file (btc_daily.csv) is saved and ready for analysis and visualization in Tableau later.\n",
    "# Quick sanity check for expected columns and row count to catch any major issues before analysis\n",
    "expected = {\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
    "missing = expected - set(btc.columns)\n",
    "if missing:\n",
    "    print(\"⚠️ Missing expected columns:\", missing)\n",
    "if len(btc) < 300:\n",
    "    print(f\"⚠️ Row count lower than expected: {len(btc)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Data Acquisition Complete\n",
    "\n",
    "I’ve finished Part 1 (Economic Indicators) and Part 2 (Cryptocurrencies) data collection, inspection, and export.\n",
    "All raw calls were inspected first (JSON/CSV preview, columns, sample rows, dtypes), then converted to tidy DataFrames and saved as CSV for downstream analysis/visualization.\n",
    "\n",
    "Files exported are:\n",
    "\n",
    " (1) gdp_quarterly.csv — Real GDP (quarterly), 2002-01-01 → 2025-04-01, 2 columns (date, value)\n",
    "\n",
    " (2) vti_daily.csv — VTI daily OHLCV, 2001-05-31 → 2025-10-17, 6 columns\n",
    "\n",
    " (3) vxx_daily.csv — VXX daily OHLCV, 2009-01-30 → 2025-10-17, 6 columns\n",
    "\n",
    " (4) btc_daily.csv — BTC-USD daily OHLCV, ~1 year (2024-11-04 → 2025-10-19), 7 columns (market_cap_usd not populated by free tier; documented)\n",
    "\n",
    "#### Note on BTC history: \n",
    "\n",
    "For Bitcoin specifically, the free Alpha Vantage daily crypto endpoint provides roughly one year of history; I tested weekly and monthly endpoints but they returned even shorter spans in my environment, so I proceeded with daily BTC and documented this limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Notebook: Wrangling, Analysis, Visuals\n",
    "I will rename this current notebook as 6.06 APIs for Financial Data_PartA_Data Retrieval.ipynb. \n",
    "\n",
    "Next, I will continue the project in a fresh notebook named 6.06_APIs for Finanical Data_PartB_Analysis_Wrangling_Visuals.ipynb. \n",
    "\n",
    "In that notebook I will load the four CSVs, perform light wrangling (date parsing, alignment, de-duplication), and create analysis fields such as returns and rolling metrics. I will then run the core analyses for Part 1 and Part 2, comparing trends (e.g., BTC vs VTI and VXX) and writing concise Markdown notes under each section to explain cleaning choices, assumptions, and findings.\n",
    "\n",
    "I will produce Python charts to support the narrative, using clear captions that summarize the key insight for each figure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tableau Companion\n",
    "I’ll also build a small Tableau workbook using the same CSVs to produce a few supporting visuals.\n",
    "\n",
    "Each chart will include a concise caption with the key takeaway consistent with the notebook analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final report (Word)\n",
    "\n",
    "Finally, I will summarize the wrangling steps, the main results for both parts, and the best charts in the provided Word document 6.06. Alpha Vantage data lab worksheet.docx. This closes the data collection phase and sets up a clean path for analysis and storytelling."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
